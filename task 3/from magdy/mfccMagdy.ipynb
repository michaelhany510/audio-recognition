{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import scipy.fftpack as fft\n",
    "from scipy.signal import get_window\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44100\n",
      "[-9.91017444e+02  1.38784362e+02  1.37784489e+02  1.27776973e+02\n",
      "  1.18441726e+02  9.97819895e+01  8.37017364e+01  6.54920344e+01\n",
      "  4.42850595e+01  2.10092922e+01  1.46949951e-12 -2.10092922e+01\n",
      " -4.42850595e+01]\n",
      "[-1.06316731e+03  1.38107877e+02  1.37842866e+02  1.27103390e+02\n",
      "  1.19440302e+02  1.00521628e+02  8.45090840e+01  6.53289367e+01\n",
      "  4.49889737e+01  2.14151634e+01  1.56577918e-12 -2.14151634e+01\n",
      " -4.49889737e+01]\n",
      "(40, 3)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = './files/magdy/'\n",
    "sample_rate, audio = wavfile.read(TRAIN_PATH + \"15.wav\")\n",
    "print(sample_rate)\n",
    "# Normalizing the audio \"dividing its values on the max value\"\n",
    "def normalize_audio(audio):\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    return audio\n",
    "\n",
    "# Calling the normalize function\n",
    "audio = normalize_audio(audio)\n",
    "\n",
    "# Dividing the audio into frames due to \"audio is not stationary\"\n",
    "def frame_audio(audio, FFT_size=2048, hop_size=512, sample_rate=44100):\n",
    "    # hop_size in ms\n",
    "    \n",
    "    audio     = np.pad(audio, int(FFT_size / 2), mode='reflect')    # return => pad : ndarray Padded array of rank equal to array with shape increased according to pad_width\n",
    "    audio = audio[:,0] #stereo to only one channel\n",
    "    frame_len = np.round(sample_rate * hop_size / 1000).astype(int) # return frame length\n",
    "    frame_num = int((len(audio) - FFT_size) / frame_len) + 1\n",
    "    frames    = np.zeros((frame_num, FFT_size))\n",
    "\n",
    "    for n in range(frame_num):\n",
    "        frames[n] = audio[n*frame_len : n*frame_len+1]\n",
    "    \n",
    "    return frames\n",
    "\n",
    "hop_size = 512 # hop is the number of points intersected among frames   # if the hop size increased \"exp 2\", then the number of frames decrease\n",
    "FFT_size = 2048\n",
    "\n",
    "audio_framed = frame_audio(audio, FFT_size=FFT_size, hop_size=hop_size, sample_rate=sample_rate)\n",
    "\n",
    "window = get_window(\"hann\", FFT_size, fftbins=True) # Create window, its size = frame size\n",
    "\n",
    "audio_win = audio_framed * window    # Multiply window by frame to make edges sync to zero\n",
    "\n",
    "audio_winT = np.transpose(audio_win) # Transpose the result\n",
    "\n",
    "audio_fft = np.empty((int(1 + FFT_size // 2), audio_winT.shape[1]), dtype=np.complex64, order='F') # Getting Fast Fourier \n",
    "\n",
    "for n in range(audio_fft.shape[1]): \n",
    "    audio_fft[:, n] = fft.fft(audio_winT[:, n], axis=0)[:audio_fft.shape[0]] # Applyting fft to all frames\n",
    "\n",
    "audio_fft = np.transpose(audio_fft)        # Transpose the Fourier\n",
    "\n",
    "audio_power = np.square(np.abs(audio_fft)) # Calculating the signal power\n",
    "\n",
    "freq_min = 0\n",
    "freq_high = sample_rate / 2\n",
    "mel_filter_num = 10\n",
    "\n",
    "def freq_to_mel(freq):\n",
    "    return 2595.0 * np.log10(1.0 + freq / 700.0) # mel equation to convert from freq scale to mel scale\n",
    "\n",
    "def met_to_freq(mels):\n",
    "    return 700.0 * (10.0**(mels / 2595.0) - 1.0) # mel equation to convert from mel scale to freq scale\n",
    "\n",
    "def get_filter_points(fmin, fmax, mel_filter_num, FFT_size, sample_rate=44100): # getting filter points\n",
    "    fmin_mel = freq_to_mel(fmin)\n",
    "    fmax_mel = freq_to_mel(fmax)\n",
    "\n",
    "    mels = np.linspace(fmin_mel, fmax_mel, num=mel_filter_num+2) # created 12 mel banks\n",
    "    freqs = met_to_freq(mels) # convert these points back to freq scale \"center frequencies of the mel band\"\n",
    "    \n",
    "    return np.floor((FFT_size + 1) / sample_rate * freqs).astype(int), freqs # round bins to nearest freq bin to \n",
    "\n",
    "filter_points, mel_freqs = get_filter_points(freq_min, freq_high, mel_filter_num, FFT_size, sample_rate=44100)\n",
    "\n",
    "def get_filters(filter_points, FFT_size): # constructing triangle filters using filter points\n",
    "    filters = np.zeros((len(filter_points)-2,int(FFT_size/2+1)))\n",
    "    \n",
    "    for n in range(len(filter_points)-2):\n",
    "        filters[n, filter_points[n] : filter_points[n + 1]] = np.linspace(0, 1, filter_points[n + 1] - filter_points[n])\n",
    "        filters[n, filter_points[n + 1] : filter_points[n + 2]] = np.linspace(1, 0, filter_points[n + 2] - filter_points[n + 1])\n",
    "    \n",
    "    return filters\n",
    "\n",
    "filters = get_filters(filter_points, FFT_size)\n",
    "\n",
    "# taken from the librosa library\n",
    "enorm = 2.0 / (mel_freqs[2:mel_filter_num+2] - mel_freqs[:mel_filter_num])\n",
    "filters *= enorm[:, np.newaxis]\n",
    "\n",
    "audio_filtered = np.dot(filters, np.transpose(audio_power))\n",
    "audio_log = 10.0 * np.log10(audio_filtered)\n",
    "\n",
    "def dct(dct_filter_num, filter_len):\n",
    "    basis = np.empty((dct_filter_num,filter_len))\n",
    "    basis[0, :] = 1.0 / np.sqrt(filter_len)\n",
    "    \n",
    "    samples = np.arange(1, 2 * filter_len, 2) * np.pi / (2.0 * filter_len)\n",
    "\n",
    "    for i in range(1, dct_filter_num):\n",
    "        basis[i, :] = np.cos(i * samples) * np.sqrt(2.0 / filter_len)\n",
    "        \n",
    "    return basis\n",
    "\n",
    "dct_filter_num = 40\n",
    "\n",
    "dct_filters = dct(dct_filter_num, mel_filter_num)\n",
    "\n",
    "cepstral_coefficents = np.dot(dct_filters, audio_log)\n",
    "\n",
    "print(np.mean(cepstral_coefficents[:13].T, axis=0))\n",
    "print(cepstral_coefficents[:13,0].T)\n",
    "print(cepstral_coefficents.shape)\n",
    "\n",
    "# #-----------------------------------------------------------------------------------------------------\n",
    "# def mfcc(*, y=None, sr=22050, S=None, n_mfcc=20, dct_type=2, norm=\"ortho\", lifter=0, **kwargs):\n",
    "#     M = scipy.fftpack.dct(S, axis=-2, type=dct_type, norm=norm)[..., :n_mfcc, :]\n",
    "\n",
    "#     if lifter > 0:\n",
    "#         # shape lifter for broadcasting\n",
    "#         LI = np.sin(np.pi * np.arange(1, 1 + n_mfcc, dtype=M.dtype) / lifter)\n",
    "#         LI = util.expand_to(LI, ndim=S.ndim, axes=-2)\n",
    "\n",
    "#         M *= 1 + (lifter / 2) * LI\n",
    "#         return M\n",
    "#     elif lifter == 0:\n",
    "#         return M\n",
    "#     else:\n",
    "#         raise ParameterError(\"MFCC lifter={} must be a non-negative number\".format(lifter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Audio Framing¶\\nBecause audio is a non stationary process, the FFT will produce distortions. To overcome this we can assume that the audio is a stationary process for a \\nshort periods of time. Because of that we devide the signal into short frames. Each audio frame will be the same size as the FFT. Also we want the frames to\\noverlap. We do that so that the frames will have some correlation between them and because we loose the information on the edges of each frame after applying\\na window function.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Audio Framing¶\n",
    "Because audio is a non stationary process, the FFT will produce distortions. To overcome this we can assume that the audio is a stationary process for a \n",
    "short periods of time. Because of that we devide the signal into short frames. Each audio frame will be the same size as the FFT. Also we want the frames to\n",
    "overlap. We do that so that the frames will have some correlation between them and because we loose the information on the edges of each frame after applying\n",
    "a window function.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Convert to frequency domain\\nNow we will convert the audio, which is currently in the time domain, to frequency domain. The FFT assumes the audio to be periodic and continues.\\nBy framing the signal we assured the audio to be periodic. To make the audio continues, we apply a window function on every frame. If we wont do that, We will\\nget high frequency distortions. To overcome this, we first need to apply a window function to the framed audio and then perforn FFT. The window assures that\\nboth ends of the signal will end close to zero.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Convert to frequency domain\n",
    "Now we will convert the audio, which is currently in the time domain, to frequency domain. The FFT assumes the audio to be periodic and continues.\n",
    "By framing the signal we assured the audio to be periodic. To make the audio continues, we apply a window function on every frame. If we wont do that, We will\n",
    "get high frequency distortions. To overcome this, we first need to apply a window function to the framed audio and then perforn FFT. The window assures that\n",
    "both ends of the signal will end close to zero.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# import numpy as np\n",
    "\n",
    "# zcr_scratch = np.array\n",
    "\n",
    "# x, sample_rate = librosa.load('./files/magdy/1.wav', res_type='kaiser_fast')\n",
    "\n",
    "# for index in range(0, len(x) - 1):\n",
    "#     zcr_scratch = zcr_scratch + (np.sign(x[index]) - np.sign(x[index + 1]) ) / 2\n",
    "\n",
    "# print(zcr_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7536d7b229462f4296d4c0dae49c3bd6cd3990c783d6b9d88ce3c31a78605890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
